{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,944,193\n",
      "Input resolution: 336\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=336, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(336, 336))\n",
       "    <function _convert_image_to_rgb at 0x7fcd3c232dd0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "import clip.clip as clip\n",
    "from PIL import Image\n",
    "import os \n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model, preprocess = clip.load(name=\"/datassd2/sswang/NFT_Search/CLIP/models/ViT-L-14-336px.pt\", device=device)\n",
    "# 将模型加载到GPU中并切换到评估模式\n",
    "# model.cuda(device).eval()\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.stack(images) 函数将一个由多个图像组成的列表 images 沿着一个新轴（默认为 0）进行连接，生成一个新的数组。这个新的数组的维度比原来的数组多了一个维度，用于存储连接后的图像。如果 images 中的每个图像的尺寸都相同，那么连接后的数组的第一个维度将是 len(images)，第二个维度将是图像的高度，第三个维度将是图像的宽度，第四个维度将是图像的通道数。\n",
    "\n",
    "这行代码是从一个图像列表中创建了一个 PyTorch 张量 image_input。np.stack(images) 函数沿着一个新轴（默认为 0）将图像列表进行连接。然后，torch.tensor() 将连接后的图像列表转换为 PyTorch 张量。.cuda() 方法将张量移动到 GPU 上进行加速运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def check_dir(dir_path):\n",
    "    \"\"\"\n",
    "    检查文件夹路径是否存在，不存在则创建\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): 待检查的文件夹路径\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        try:\n",
    "            os.makedirs(dir_path)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "def load_json(json_path):\n",
    "    \"\"\"\n",
    "    以只读的方式打开json文件\n",
    "\n",
    "    Args:\n",
    "        config_path: json文件路径\n",
    "\n",
    "    Returns:\n",
    "        A dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='UTF-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_json(save_path, data):\n",
    "    \"\"\"\n",
    "    Saves the data to a file with the given filename in the given path\n",
    "\n",
    "    Args:\n",
    "        :param save_path: The path to the folder where you want to save the file\n",
    "        :param filename: The name of the file to save\n",
    "        :param data: The data to be saved\n",
    "\n",
    "    \"\"\"\n",
    "    with open(save_path, 'w', encoding='UTF-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def is_str_Length_valid(str_list) -> bool:\n",
    "    \"\"\"\n",
    "    判断字符串长度是否超过77\n",
    "\n",
    "    Args:\n",
    "        str_list (list): 字符串列表\n",
    "\n",
    "    Returns:\n",
    "        bool: 是否超过77\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for str in str_list:\n",
    "            clip.tokenize(str)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def tensorlize_imgs(model, img_path_list) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    使用模型提取图片特征，返回图片特征向量列表\n",
    "\n",
    "    Args:\n",
    "        img_path_list (list): 图片路径列表\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 图片特征向量列表\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    for img_path in img_path_list:\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "            # 首先将图片预处理成模型需要的格式\n",
    "        images.append(preprocess(image))\n",
    "        # 把图片加载进cuda中\n",
    "    image_input = torch.tensor(np.stack(images)).cuda(device=device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input).float()\n",
    "        # 将image_features从GPU移动到CPU，并返回\n",
    "        return image_features.cpu()\n",
    "            \n",
    "\n",
    "def tensorlize_texts(model, text_tokens_list) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    使用模型提取文本特征，返回文本特征向量列表\n",
    "\n",
    "    Args:\n",
    "        text_tokens_list (list): 文本列表\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 文本特征向量列表\n",
    "    \"\"\"\n",
    "    text_tokens = clip.tokenize(text_tokens_list).cuda(device=device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        # 将text_features从GPU移动到CPU，并返回\n",
    "        return text_features.cpu().numpy().tolist()\n",
    "    \n",
    "def load_img_tensor(device, imgTensor_path):\n",
    "    \"\"\"\n",
    "    加载图片的tensor 向量到指定cuda中\n",
    "\n",
    "    Args:\n",
    "        device (str): cuda\n",
    "        img_path (str): 图片tensor路径\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 图片特征向量\n",
    "    \"\"\"\n",
    "    # 加载json文件\n",
    "    NFT_tensor_data = load_json(imgTensor_path)\n",
    "    image_features = NFT_tensor_data['image_features']\n",
    "    image_tensors = torch.tensor(image_features).to(device)\n",
    "    return image_tensors\n",
    "\n",
    "def load_des_tensor(device, desTensor_path):\n",
    "    \"\"\"\n",
    "    加载描述的tensor 向量到指定cuda中\n",
    "\n",
    "    Args:\n",
    "        device (str): cuda\n",
    "        img_path (str): 描述的tensor路径\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 图片特征向量\n",
    "    \"\"\"\n",
    "    # 加载json文件\n",
    "    NFT_tensor_data = load_json(desTensor_path)\n",
    "    des_tensors = [list(x) for x in zip(*NFT_tensor_data['des_tensors'])]\n",
    "    des_features = torch.tensor(des_tensors).to(device)\n",
    "    return des_features\n",
    "\n",
    "def slide_window_tokenizer(text, window_size, step_size) -> list:\n",
    "    \"\"\"\n",
    "    为了处理长度超过77的句子，这里设计滑动窗口分词\n",
    "\n",
    "    Args:\n",
    "        text (str): 将要被拆分的句子\n",
    "\n",
    "    Returns:\n",
    "        list: 拆分后的句子列表\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    sentences = []\n",
    "    slide_window_list = [i for i in range(0, len(words) - 1, step_size) if i + step_size < len(words) - 1]\n",
    "    for i in slide_window_list:\n",
    "        sentence = ' '.join(words[i:i+window_size])\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def calculate_cosine_similarity_topk(img_features, des_features, k = 10) -> tuple:\n",
    "    \"\"\"\n",
    "    计算图片特征和描述特征的余弦相似度，并返回topk的结果\n",
    "\n",
    "    Args:\n",
    "        img_features (torch.tensor): 图像特征向量\n",
    "        des_features (torch.tensor): 描述特征向量\n",
    "        k (int, optional): 前k位结果. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (topk的相似度，topk的索引)\n",
    "    \"\"\"\n",
    "    # 归一化图片特征\n",
    "    img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # 归一化描述特征\n",
    "    des_features /= des_features.norm(dim=-1, keepdim=True)\n",
    "    # similarity = des_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "    # 计算余弦相似度\n",
    "    text_probs = (100.0 * img_features @ des_features.T).softmax(dim=-1)\n",
    "    top_probs, top_labels = text_probs.cpu().topk(k, dim=-1)\n",
    "    return top_probs, top_labels\n",
    "\n",
    "\n",
    "def slide_window_tokenizer(text, window_size, step_size) -> list:\n",
    "    \"\"\"\n",
    "    为了处理长度超过77的句子，这里设计滑动窗口分词\n",
    "\n",
    "    Args:\n",
    "        text (str): 将要被拆分的句子\n",
    "\n",
    "    Returns:\n",
    "        list: 拆分后的句子列表\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    sentences = []\n",
    "    slide_window_list = [i for i in range(0, len(words) - 1, step_size) if i + step_size < len(words) - 1]\n",
    "    for i in slide_window_list:\n",
    "        sentence = ' '.join(words[i:i+window_size])\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def tensorlize_texts_slideWindow(model, text, window_size, step_size):\n",
    "    \"\"\"\n",
    "    将长文本分块，然后对每块文本进行向量化，最后将这些向量平均。\n",
    "\n",
    "    Args:\n",
    "        model (CLIP): 使用的 CLIP 模型。\n",
    "        text (str): 输入的文本。\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 文本特征向量。\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = slide_window_tokenizer(text, window_size, step_size)\n",
    "    tensor_list = []\n",
    "    for chunk in chunks:\n",
    "        tokens = clip.tokenize([chunk]).cuda(device=device)\n",
    "        with torch.no_grad():\n",
    "            tensor_list.append(model.encode_text(tokens).float().cpu())\n",
    "    # 使用所有块的平均值作为文本的表示\n",
    "    avg_tensor = torch.mean(torch.stack(tensor_list), dim=0)\n",
    "    return avg_tensor.numpy().tolist()\n",
    "\n",
    "\n",
    "def tensorlize_valid_subsentence(model, text) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    截取有效的子句，然后求特征向量值\n",
    "\n",
    "    Args:\n",
    "        model (CLIP): 使用的 CLIP 模型。\n",
    "        text (str): 输入的文本。\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 文本特征向量。\n",
    "    \"\"\"\n",
    "    text_tensor = None\n",
    "    # 标记为False时，表示该句子无法被模型处理，需要进行拆分\n",
    "    flag = False\n",
    "    words = text.split()\n",
    "    text_length = len(words)\n",
    "    while not flag:\n",
    "        try:\n",
    "            text_tensor = tensorlize_texts(model, text)\n",
    "            flag = True\n",
    "        except:\n",
    "            text_length -= 1\n",
    "            text = ' '.join(words[:text_length])\n",
    "    return text_tensor\n",
    "\n",
    "def handle_long_texts(model, text_list, window_size, step_size) -> list:\n",
    "    \"\"\"\n",
    "    处理长文本，将长文本分块，然后对每块文本进行向量化，最后将这些向量平均。\n",
    "\n",
    "    Args:\n",
    "        model (CLIP): 使用的 CLIP 模型。\n",
    "        text_list (list): 输入的文本列表。\n",
    "        window_size (int): 窗口宽度\n",
    "        step_size (int): 窗口移动的步长\n",
    "\n",
    "    Returns:\n",
    "        list: 文本特征向量列表。\n",
    "    \"\"\"\n",
    "    text_tensor_list = []\n",
    "    for text in text_list:\n",
    "        try:\n",
    "            text_tensor_list.append(tensorlize_texts(model, text))\n",
    "        except:\n",
    "            # text_tensor_list.append(tensorlize_texts_slideWindow(model, text, window_size, step_size))\n",
    "            text_tensor_list.append(tensorlize_valid_subsentence(model, text))\n",
    "    return text_tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path = Path(\"/datassd2/sswang/dataset/mini100/\")\n",
    "target_dataset_path = Path(\"/datassd2/sswang/dataset/mini100_tensor_V2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理： CryptoPunks ...\n",
      "处理完成： CryptoPunks\n",
      "开始处理： BoredApeYachtClub ...\n",
      "处理完成： BoredApeYachtClub\n",
      "开始处理： MutantApeYachtClub ...\n",
      "处理完成： MutantApeYachtClub\n",
      "开始处理： Azuki ...\n",
      "处理完成： Azuki\n",
      "开始处理： CLONEX ...\n",
      "处理完成： CLONEX\n",
      "开始处理： Moonbirds ...\n",
      "处理完成： Moonbirds\n",
      "开始处理： Doodles ...\n",
      "处理完成： Doodles\n",
      "开始处理： BoredApeKennelClub ...\n",
      "处理完成： BoredApeKennelClub\n",
      "开始处理： Meebits ...\n",
      "处理完成： Meebits\n",
      "开始处理： PudgyPenguins ...\n",
      "处理完成： PudgyPenguins\n",
      "开始处理： Cool Cats ...\n",
      "处理完成： Cool Cats\n",
      "开始处理： Beanz ...\n",
      "处理完成： Beanz\n",
      "开始处理： MechMinds ...\n",
      "处理完成： MechMinds\n",
      "开始处理： World of Women ...\n",
      "处理完成： World of Women\n",
      "开始处理： CrypToadz ...\n",
      "处理完成： CrypToadz\n",
      "开始处理： 0N1 Force ...\n",
      "处理完成： 0N1 Force\n",
      "开始处理： mfers ...\n",
      "处理完成： mfers\n",
      "开始处理： Karafuru ...\n",
      "处理完成： Karafuru\n",
      "开始处理： HAPE PRIME ...\n",
      "处理完成： HAPE PRIME\n",
      "开始处理： MekaVerse ...\n",
      "处理完成： MekaVerse\n",
      "开始处理： projectPXN ...\n",
      "处理完成： projectPXN\n",
      "开始处理： FLUF ...\n",
      "处理完成： FLUF\n",
      "开始处理： Hashmasks ...\n",
      "处理完成： Hashmasks\n",
      "开始处理： Moonbirds Oddities ...\n",
      "处理完成： Moonbirds Oddities\n",
      "开始处理： Creature World ...\n",
      "处理完成： Creature World\n",
      "开始处理： 3Landers ...\n",
      "处理完成： 3Landers\n",
      "开始处理： Phanta Bear ...\n",
      "处理完成： Phanta Bear\n",
      "开始处理： CyberKongz VX ...\n",
      "处理完成： CyberKongz VX\n",
      "开始处理： Deadfellaz ...\n",
      "处理完成： Deadfellaz\n",
      "开始处理： KaijuKingz ...\n",
      "处理完成： KaijuKingz\n",
      "开始处理： VeeFriends Series 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sswang/anaconda3/envs/NFT_search/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成： VeeFriends Series 2\n",
      "开始处理： Lazy Lions ...\n",
      "处理完成： Lazy Lions\n",
      "开始处理： World of Women Galaxy ...\n",
      "处理完成： World of Women Galaxy\n",
      "开始处理： ALIENFRENS ...\n",
      "处理完成： ALIENFRENS\n",
      "开始处理： Prime Ape Planet ...\n",
      "处理完成： Prime Ape Planet\n",
      "开始处理： The Doge Pound ...\n",
      "处理完成： The Doge Pound\n",
      "开始处理： Sappy Seals ...\n",
      "处理完成： Sappy Seals\n",
      "开始处理： CyberKongz ...\n",
      "处理完成： CyberKongz\n",
      "开始处理： DigiDaigaku ...\n",
      "处理完成： DigiDaigaku\n",
      "开始处理： CoolmansUniverse ...\n",
      "处理完成： CoolmansUniverse\n",
      "开始处理： VOX Series 1 ...\n",
      "处理完成： VOX Series 1\n",
      "开始处理： Capsule ...\n",
      "处理完成： Capsule\n",
      "开始处理： Murakami.Flowers ...\n",
      "处理完成： Murakami.Flowers\n",
      "开始处理： SupDucks ...\n",
      "处理完成： SupDucks\n",
      "开始处理： Valhalla ...\n",
      "处理完成： Valhalla\n",
      "开始处理： DEGEN TOONZ ...\n",
      "处理完成： DEGEN TOONZ\n",
      "开始处理： Lives of Asuna ...\n",
      "处理完成： Lives of Asuna\n",
      "开始处理： Nakamigos ...\n",
      "处理完成： Nakamigos\n",
      "开始处理： Sneaky Vampire Syndicate ...\n",
      "处理完成： Sneaky Vampire Syndicate\n",
      "开始处理： Killer GF ...\n",
      "处理完成： Killer GF\n",
      "开始处理： Adam Bomb Squad ...\n",
      "处理完成： Adam Bomb Squad\n",
      "开始处理： Impostors Genesis ...\n",
      "处理完成： Impostors Genesis\n",
      "开始处理： CryptoSkulls ...\n",
      "处理完成： CryptoSkulls\n",
      "开始处理： MURI ...\n",
      "处理完成： MURI\n",
      "开始处理： Weirdo Ghost Gang ...\n",
      "处理完成： Weirdo Ghost Gang\n",
      "开始处理： KILLABEARS ...\n",
      "处理完成： KILLABEARS\n",
      "开始处理： Acclimated​MoonCats ...\n",
      "处理完成： Acclimated​MoonCats\n",
      "开始处理： Milady ...\n",
      "处理完成： Milady\n",
      "开始处理： Chimpers ...\n",
      "处理完成： Chimpers\n",
      "开始处理： My Pet Hooligan ...\n",
      "处理完成： My Pet Hooligan\n",
      "开始处理： RumbleKongLeague ...\n",
      "处理完成： RumbleKongLeague\n",
      "开始处理： Jungle Freaks ...\n",
      "处理完成： Jungle Freaks\n",
      "开始处理： MetaHero ...\n",
      "处理完成： MetaHero\n",
      "开始处理： Bears Deluxe ...\n",
      "处理完成： Bears Deluxe\n",
      "开始处理： Lil Heroes ...\n",
      "处理完成： Lil Heroes\n",
      "开始处理： Quirkies ...\n",
      "处理完成： Quirkies\n",
      "开始处理： tubby cats ...\n",
      "处理完成： tubby cats\n",
      "开始处理： Galactic Apes ...\n",
      "处理完成： Galactic Apes\n",
      "开始处理： a KID called BEAST ...\n",
      "处理完成： a KID called BEAST\n",
      "开始处理： OnChainMonkey ...\n",
      "处理完成： OnChainMonkey\n",
      "开始处理： CryptoBatz by Ozzy Osbourne ...\n",
      "处理完成： CryptoBatz by Ozzy Osbourne\n",
      "开始处理： LilPudgys ...\n",
      "处理完成： LilPudgys\n",
      "开始处理： MutantCats ...\n",
      "处理完成： MutantCats\n",
      "开始处理： ForgottenRunesWizardsCult ...\n",
      "处理完成： ForgottenRunesWizardsCult\n",
      "开始处理： Boss Beauties ...\n",
      "处理完成： Boss Beauties\n",
      "开始处理： Keepers V2 ...\n",
      "处理完成： Keepers V2\n",
      "开始处理： Anonymice ...\n",
      "处理完成： Anonymice\n",
      "开始处理： Kiwami ...\n",
      "处理完成： Kiwami\n",
      "开始处理： HypeBears ...\n",
      "处理完成： HypeBears\n",
      "开始处理： Akutars ...\n",
      "处理完成： Akutars\n",
      "开始处理： GalaxyEggs ...\n",
      "处理完成： GalaxyEggs\n",
      "开始处理： SHIBOSHIS ...\n",
      "处理完成： SHIBOSHIS\n",
      "开始处理： CryptoMories ...\n",
      "处理完成： CryptoMories\n",
      "开始处理： Hero ...\n",
      "处理完成： Hero\n",
      "开始处理： Fighter ...\n",
      "处理完成： Fighter\n",
      "开始处理： C-01 Official Collection ...\n",
      "处理完成： C-01 Official Collection\n",
      "开始处理： Crypto Bull Society ...\n",
      "处理完成： Crypto Bull Society\n",
      "开始处理： Chain Runners ...\n",
      "处理完成： Chain Runners\n",
      "开始处理： KIA ...\n",
      "处理完成： KIA\n",
      "开始处理： MOAR by Joan Cornella ...\n",
      "处理完成： MOAR by Joan Cornella\n",
      "开始处理： OxyaOriginProject ...\n",
      "处理完成： OxyaOriginProject\n",
      "开始处理： Genuine Undead ...\n",
      "处理完成： Genuine Undead\n",
      "开始处理： The Humanoids ...\n",
      "处理完成： The Humanoids\n",
      "开始处理： The Heart Project ...\n",
      "处理完成： The Heart Project\n",
      "开始处理： inbetweeners ...\n",
      "处理完成： inbetweeners\n",
      "开始处理： Sevens Token ...\n",
      "处理完成： Sevens Token\n",
      "开始处理： Kanpai Pandas ...\n",
      "处理完成： Kanpai Pandas\n",
      "开始处理： Owls ...\n",
      "处理完成： Owls\n",
      "开始处理： WonderPals ...\n",
      "处理完成： WonderPals\n",
      "开始处理： Groupies ...\n",
      "处理完成： Groupies\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import copy\n",
    "\n",
    "NFT_list_dict = load_json(dataset_base_path.joinpath(\"NFT_list.json\"))\n",
    "\n",
    "collection_list_copy = copy.deepcopy(NFT_list_dict[\"collection_list_copy\"])\n",
    "\n",
    "for key, value in collection_list_copy.items():\n",
    "    NFT_collection_path = dataset_base_path.joinpath(value)\n",
    "    print(\"开始处理：\", NFT_collection_path.name, \"...\")\n",
    "    check_dir(target_dataset_path.joinpath(NFT_collection_path.name))\n",
    "\n",
    "    NFT_tensor_data = {}\n",
    "    img_path_list = list(NFT_collection_path.joinpath(\"img\").iterdir())\n",
    "    # 提取图片名称\n",
    "    img_name_list = [img_path.stem for img_path in img_path_list]\n",
    "\n",
    "    # 提取图片特征向量\n",
    "    image_features_CPU = tensorlize_imgs(model, img_path_list)\n",
    "\n",
    "    # 提取文本特征向量\n",
    "    des_tensor = []\n",
    "\n",
    "    des_query_dict = load_json(NFT_collection_path.joinpath(\"description.json\"))\n",
    "\n",
    "    for img_name in img_name_list:\n",
    "        # 去掉 description 中的序号（1. 2. 3.）\n",
    "        des_list = [re.sub(r'^\\d+\\. ', '', des) for des in des_query_dict[img_name]]\n",
    "        # 判断描述的长度有没有超过77\n",
    "        if is_str_Length_valid(des_list) == False:\n",
    "            # 如果超过77，就使用滑动窗口分词\n",
    "            des_tensor.append(handle_long_texts(model, des_list, 50, 20))\n",
    "        else:\n",
    "            des_tensor.append(tensorlize_texts(model, des_list))\n",
    "\n",
    "    NFT_tensor_data[\"img_name_list\"] = img_name_list\n",
    "    NFT_tensor_data[\"image_features\"] = image_features_CPU.numpy().tolist()\n",
    "    NFT_tensor_data[\"des_tensors\"] = des_tensor\n",
    "    save_json(target_dataset_path.joinpath(NFT_collection_path.name, \"NFT_tensor_data.json\"), NFT_tensor_data)\n",
    "    print(\"处理完成：\", NFT_collection_path.name)\n",
    "    \n",
    "    # 将已经处理完成的项目从列表中删除\n",
    "    del NFT_list_dict[\"collection_list_copy\"][key]\n",
    "    # 将处理完成的项目列表保存到json文件中\n",
    "    save_json(dataset_base_path.joinpath(\"NFT_list.json\"), NFT_list_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算图像特征和描述特征的余弦相似度\n",
    "image_features = load_img_tensor(device, target_dataset_path.joinpath(\"Prime Ape Planet\", \"NFT_tensor_data.json\"))\n",
    "des_features = load_des_tensor(device, target_dataset_path.joinpath(\"Prime Ape Planet\", \"NFT_tensor_data.json\"))\n",
    "des_features1, des_features2, des_feature3 = des_features\n",
    "top_probs, top_labels = calculate_cosine_similarity_topk(image_features, des_features1, 10)\n",
    "print(top_probs.shape)\n",
    "print(top_probs)\n",
    "print(top_labels.shape)\n",
    "print(top_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.295318603515625, 0.4466552734375, -0.07173919677734375, 0.2760963439941406, 0.3887939453125, -0.59649658203125, -0.046417236328125, 0.4308662414550781, -0.075347900390625, 0.325439453125, 0.0817413330078125, -0.1146240234375, -0.04766082763671875, -0.0941925048828125, -0.21435546875, 0.02606201171875, 0.373687744140625, -0.107086181640625, 0.05908942222595215, -0.501800537109375, 0.3129425048828125, 0.059429168701171875, 0.35211181640625, 0.110443115234375, 0.188812255859375, -0.057033538818359375, -0.52545166015625, 0.26125335693359375, 0.04547119140625, -0.31732177734375, 0.24681854248046875, 0.156768798828125, -0.246673583984375, -0.10150527954101562, 0.6119384765625, 0.16094970703125, 0.472076416015625, -0.0641334056854248, -0.10406494140625, -0.3747711181640625, 0.0521240234375, -0.090972900390625, 0.4623870849609375, 0.2958984375, -0.37103271484375, 0.06674909591674805, 0.4700927734375, -0.16729736328125, -0.356719970703125, 0.0161285400390625, 0.3125, -0.0677947998046875, 0.1024627685546875, 0.352752685546875, 0.17549896240234375, 0.29681396484375, 0.2157440185546875, 0.24983978271484375, 0.471435546875, 0.27386474609375, 0.17532730102539062, 0.4249725341796875, -0.406463623046875, 0.00592041015625, -0.0513153076171875, -0.34820556640625, 0.4652385711669922, -0.03321075439453125, 0.32568359375, -0.01343536376953125, 0.008907318115234375, -0.633880615234375, 0.14678955078125, 0.1350860595703125, 0.27777099609375, 0.10314178466796875, 0.614990234375, -0.0382080078125, 0.25650787353515625, -0.1045379638671875, 0.04987144470214844, 0.06269073486328125, -0.20343780517578125, -0.54412841796875, -0.21482086181640625, -0.2378082275390625, -0.3180389404296875, -0.098114013671875, 0.0930938720703125, -0.0371856689453125, 0.07721138000488281, -0.077850341796875, 0.5294189453125, 0.15004730224609375, -0.1427459716796875, 0.47247314453125, -0.171051025390625, -0.08728790283203125, 0.32178497314453125, -0.4825439453125, -0.33332061767578125, -0.45501708984375, -0.305145263671875, -0.069305419921875, -0.03232383728027344, 0.15997314453125, -0.15020751953125, -0.0042743682861328125, -0.0205535888671875, 0.0569915771484375, -0.20471954345703125, -0.04042816162109375, -0.2200469970703125, -0.08254814147949219, 0.2401123046875, -0.1357574462890625, -0.427734375, 0.1112060546875, 0.219482421875, 0.223114013671875, -0.0622711181640625, -0.112030029296875, 0.51470947265625, 0.012516021728515625, 0.2524566650390625, 0.23056602478027344, 0.036712646484375, 0.158355712890625, -0.1379547119140625, -0.0919342041015625, 0.117431640625, -0.550262451171875, 0.20232009887695312, 0.267242431640625, 0.16558837890625, 0.133209228515625, 0.22263336181640625, 0.006855010986328125, 0.5150146484375, -0.124755859375, -0.15644073486328125, 0.371795654296875, -0.13470458984375, 0.14310455322265625, 0.2346343994140625, 0.2077045440673828, -0.478424072265625, 0.4369659423828125, -0.059234619140625, 0.12668991088867188, 0.2454071044921875, 0.2284088134765625, 0.192352294921875, -0.178009033203125, -0.2935791015625, 0.0433197021484375, -0.1556243896484375, 0.3318672180175781, -0.090789794921875, -0.28211212158203125, 0.059661865234375, 0.11042284965515137, -0.0062408447265625, 0.476104736328125, 0.039879798889160156, -0.14723968505859375, -0.489532470703125, 0.25469970703125, 0.056549072265625, 0.10302734375, 0.259613037109375, -0.00136566162109375, 0.3936309814453125, -0.219482421875, 0.360015869140625, 0.18804931640625, 0.113433837890625, -0.987152099609375, -0.38124847412109375, -0.08568954467773438, 0.2951812744140625, 0.0162353515625, 0.26275634765625, 0.26700592041015625, -0.010650634765625, -0.1609649658203125, 0.07568359375, -0.339263916015625, -0.028438568115234375, 0.3874053955078125, -0.06561279296875, 0.5076904296875, -0.41162109375, -0.6722412109375, 0.0321197509765625, -1.90887451171875, 0.20758056640625, 0.165557861328125, -0.2825927734375, 0.355072021484375, 0.262115478515625, -1.70208740234375, -0.1993255615234375, 0.18454742431640625, -0.0416107177734375, -0.11250591278076172, 0.0461578369140625, 0.3198394775390625, -0.49700927734375, -0.4261016845703125, 0.28424072265625, 0.02788543701171875, -0.37224578857421875, -0.29302978515625, -0.2022247314453125, -0.171630859375, -0.2999153137207031, 0.14981818199157715, 0.058902740478515625, -0.187652587890625, -0.09136199951171875, -0.21905517578125, 0.07724952697753906, 0.1438751220703125, 0.10988998413085938, -0.104949951171875, -0.0435333251953125, -0.35540771484375, 0.1767730712890625, 0.14882278442382812, -0.41219329833984375, 0.00890350341796875, -0.129974365234375, -0.190338134765625, -0.01812744140625, -0.20830917358398438, -0.57159423828125, -0.19152069091796875, 0.275604248046875, 0.11038589477539062, 0.14153289794921875, -0.2602996826171875, -0.0020599365234375, -0.48968505859375, -0.2584686279296875, 0.4280853271484375, -0.2730712890625, -0.042572021484375, -0.21350860595703125, -0.491851806640625, -0.17364501953125, -0.16005706787109375, 0.404571533203125, -0.198944091796875, -0.01470947265625, 0.21331787109375, 0.019750595092773438, 0.030994415283203125, 0.22711181640625, -0.9447021484375, 0.172088623046875, 0.0971832275390625, -0.064117431640625, 0.43359375, -0.13753509521484375, 0.132659912109375, 0.436248779296875, 0.0692291259765625, -0.00943756103515625, -0.0755615234375, 0.3324737548828125, 0.201263427734375, 0.1513214111328125, -0.229766845703125, -0.048309326171875, -0.4066314697265625, 0.13812255859375, -0.163848876953125, -0.047210693359375, -0.347747802734375, -0.15828800201416016, -0.0362548828125, 0.08795547485351562, -0.48937225341796875, 0.1411285400390625, -0.1335277557373047, 0.4048919677734375, -0.21985626220703125, -1.53057861328125, -0.073028564453125, -0.24290943145751953, -0.4476318359375, -0.339752197265625, 0.10866165161132812, -0.4995574951171875, -0.3350372314453125, -0.046352386474609375, 0.00191497802734375, -0.057586669921875, -0.14183807373046875, -0.055755615234375, 0.165679931640625, 0.43310546875, -0.38018798828125, -0.203704833984375, -0.1263742446899414, 0.20904541015625, 0.250640869140625, 0.4029541015625, 0.3698844909667969, 0.287109375, 0.10486984252929688, -0.14522933959960938, -0.1025238037109375, -0.08356094360351562, 0.0506591796875, -7.652587890625, -0.26361083984375, 0.020355224609375, -0.20627593994140625, -0.0770263671875, -0.2855224609375, -0.283935546875, 0.352691650390625, -0.65679931640625, 0.008625030517578125, -0.3087615966796875, 0.0462646484375, -0.0916748046875, 0.55780029296875, -0.2081756591796875, 0.324127197265625, -0.070159912109375, 0.285064697265625, 0.0975799560546875, 0.1835174560546875, 0.15830230712890625, -0.12591552734375, -0.19976806640625, -0.333740234375, -0.020050048828125, 0.0797576904296875, 0.38146209716796875, -0.3599700927734375, -0.5176239013671875, 0.2235565185546875, 0.10428619384765625, 0.120086669921875, 0.31829833984375, -0.0273590087890625, 0.067047119140625, -0.32927894592285156, -0.11324310302734375, 0.037750244140625, 0.47454071044921875, -0.086151123046875, 0.46728515625, -0.0629730224609375, -0.31780242919921875, 0.12481307983398438, 0.0961151123046875, -0.0314483642578125, 0.255523681640625, 1.538726806640625, -0.08426284790039062, -0.590576171875, 0.3152618408203125, -0.4036865234375, 0.3939208984375, -0.8541259765625, -0.06831121444702148, -0.183929443359375, -0.13224411010742188, 0.07988739013671875, -0.306549072265625, -0.08139991760253906, 0.6282958984375, -0.334930419921875, 0.2236480712890625, -0.559326171875, -0.21666717529296875, -0.0378265380859375, -0.0568084716796875, 0.12693405151367188, -0.04833984375, 0.303924560546875, 0.25347900390625, -0.31683349609375, -0.294586181640625, 0.474609375, -0.15989327430725098, -0.24341773986816406, -0.15643310546875, -0.3896331787109375, -0.500640869140625, 0.48162841796875, 0.1028289794921875, 0.4520120620727539, 0.2392435073852539, 0.16442108154296875, -0.1038818359375, 0.383941650390625, 0.23884057998657227, -0.31646728515625, 0.46990966796875, 0.507904052734375, -0.281402587890625, -0.3196258544921875, 0.195465087890625, 0.012117862701416016, -0.16272735595703125, -0.3067626953125, -0.16033363342285156, -0.98486328125, 0.44744873046875, 0.2984600067138672, -0.19858169555664062, 0.06842041015625, 0.302154541015625, -0.089263916015625, -0.44370174407958984, 0.062166452407836914, 0.22802734375, -0.1064453125, -0.0958251953125, 0.293670654296875, 0.003902435302734375, -0.12478256225585938, 0.3992919921875, -0.05523681640625, 0.4004364013671875, 0.240631103515625, 0.0660247802734375, 0.398651123046875, -0.218048095703125, -0.33758544921875, 0.18463134765625, 0.027099609375, -0.025712966918945312, 0.24774932861328125, 6.20263671875, 0.14764404296875, 0.0187530517578125, 0.137420654296875, -0.4268646240234375, -0.2547607421875, -0.3218994140625, 0.024078369140625, -0.0587158203125, -0.39349365234375, 0.71124267578125, -0.13541412353515625, -0.07691764831542969, -0.47357177734375, 0.08287811279296875, 0.409759521484375, -0.27197265625, -0.1695556640625, -0.2586669921875, 0.112701416015625, -0.090179443359375, -0.15071868896484375, 0.2103118896484375, 0.3300323486328125, -0.370025634765625, -0.15726470947265625, 0.494354248046875, 0.941162109375, -0.255340576171875, 0.443359375, -0.1161651611328125, -0.15744400024414062, 0.30342864990234375, -0.2349395751953125, 0.5238037109375, 0.2170562744140625, -0.0965423583984375, 0.477874755859375, 0.16384470462799072, 0.532440185546875, 0.08642578125, -0.323638916015625, -0.0699462890625, -0.1575164794921875, -0.07489013671875, 0.09014892578125, 0.47796630859375, -0.07898712158203125, -0.03704833984375, -0.19311904907226562, -0.13166046142578125, 0.023590087890625, 0.0964508056640625, 0.73992919921875, 0.020111083984375, 0.06668853759765625, -0.12069416046142578, -0.06568717956542969, 0.20653533935546875, 0.304229736328125, 0.14952850341796875, -0.2989349365234375, -0.372833251953125, -0.17889404296875, -0.3206787109375, -0.08974075317382812, 0.0311431884765625, 0.14647674560546875, 0.514373779296875, 0.0435333251953125, -0.1644744873046875, -0.032958984375, 0.56182861328125, 0.14475250244140625, -0.1930084228515625, -0.44476318359375, 0.1724700927734375, -0.19815826416015625, -0.1368255615234375, -0.129730224609375, 0.4737548828125, -0.604248046875, 0.02925872802734375, 0.12896728515625, 0.4404296875, -0.06411552429199219, 0.316802978515625, 0.2815704345703125, -0.2701416015625, -0.1182403564453125, 0.2879486083984375, 0.0152435302734375, 0.350677490234375, -0.2923717498779297, 0.3952789306640625, 0.0587310791015625, 0.114837646484375, 0.21533215045928955, -0.07690811157226562, -0.1043243408203125, 0.15692138671875, -0.18011188507080078, -0.418212890625, 0.163330078125, 0.08494186401367188, -0.0247802734375, -0.04189300537109375, -0.12786865234375, 0.16424560546875, 0.1182861328125, -0.13671875, 0.0050811767578125, -0.277587890625, -0.2827157974243164, -0.51312255859375, -0.07957839965820312, 0.23290252685546875, -0.28607177734375, -0.60491943359375, 0.34261131286621094, 0.61895751953125, 0.12601470947265625, -0.0146636962890625, -0.07330465316772461, -0.383026123046875, 0.2113800048828125, -0.420928955078125, -0.168060302734375, -0.10882568359375, -0.08519363403320312, 0.09026408195495605, 0.18963623046875, -0.2143402099609375, -0.03983497619628906, 0.08832550048828125, -0.484161376953125, 0.022369384765625, 0.1260986328125, 0.2927093505859375, 0.701416015625, 0.6343994140625, -0.171356201171875, 0.2473602294921875, 0.0211334228515625, 0.25518798828125, 0.1595287322998047, -0.16351318359375, 0.22081756591796875, 0.18579864501953125, 0.4195556640625, -0.592529296875, -0.2140960693359375, 0.396942138671875, -0.326324462890625, -0.12689208984375, 0.26251220703125, 0.62799072265625, -0.4139404296875, 0.207061767578125, -0.2506256103515625, -0.3084259033203125, -0.0739288330078125, 0.030834197998046875, 0.29144287109375, -0.36865234375, -0.57391357421875, 0.060714006423950195, 0.4308013916015625, -0.1508026123046875, -0.03324127197265625, -0.0334625244140625, -0.246490478515625, -0.2889556884765625, 0.31463623046875, -0.027280807495117188, -0.78955078125, 0.1348876953125, -0.05819129943847656, -0.1669464111328125, 0.008514404296875, 0.0350341796875, -0.206787109375, 0.16611480712890625, -0.020130157470703125, -0.1974029541015625, 1.0130615234375, -0.58905029296875, 0.458587646484375, 0.40423583984375, -0.62591552734375, 0.309722900390625, 0.01995849609375, 0.065032958984375, 0.07611846923828125, -0.4188232421875, -0.283538818359375, -0.357177734375, -0.03263092041015625, 0.60028076171875, 0.12351226806640625, 0.36297607421875, -0.057262420654296875, 0.24321746826171875, -0.1885986328125, 0.99774169921875, 0.452880859375, -0.2853546142578125, -0.4927558898925781, -0.020938873291015625, -0.22114181518554688, 0.13918685913085938, -0.13102340698242188, -0.124176025390625, 0.2874603271484375, -0.28073883056640625, -0.196014404296875, -0.175811767578125, -0.17323684692382812, 0.16586685180664062, 0.63525390625, 0.192596435546875, 0.1411285400390625, -0.329437255859375, -0.04315185546875, -0.2363128662109375, 0.048656463623046875, 0.3146820068359375, -0.016864776611328125, 0.091583251953125, 0.9005126953125, -0.1624908447265625, -0.25649261474609375, 0.0127410888671875, 0.117828369140625, -0.1348876953125, -0.08599853515625, -0.1185455322265625, -0.218353271484375, 0.09228324890136719, -0.19974517822265625, -0.0470733642578125, 0.2396554946899414, 0.2195892333984375, -0.18053209781646729, -0.01747894287109375, 0.06696319580078125, 0.2692718505859375, 0.0928955078125, -0.163330078125, 0.2838897705078125, 0.370452880859375, -0.115753173828125, 0.30084228515625, 0.118072509765625, -0.011871337890625, 0.057403564453125, 0.0403900146484375, 0.316009521484375, -0.09882164001464844, 0.4872589111328125, 0.05702972412109375, 0.24114990234375, 0.25627994537353516, -1.56591796875, 0.52630615234375, 0.252777099609375, -0.07166481018066406, 0.075836181640625, 0.2662200927734375, 0.21970367431640625, -0.0349578857421875, 0.063873291015625, -0.24733734130859375, -0.8465576171875, 0.38848876953125, -0.02947998046875, 0.5238037109375, -0.1808929443359375, 0.24666976928710938, 0.0826416015625, -0.45672607421875, -0.055080413818359375, -0.2264404296875, -0.54931640625, 0.4102935791015625, -0.4675445556640625, 0.210174560546875, -0.12860107421875, 0.24542236328125, 0.46392822265625, 0.0066680908203125, 0.3912200927734375, 0.04010009765625, -0.10247802734375, 0.1594390869140625, -0.228668212890625, -0.193328857421875, 0.44122314453125, -0.11933708190917969, -0.12282156944274902, -0.07597064971923828, 0.2095947265625, 0.11383056640625, -0.056549072265625, 0.2674560546875, -0.027971267700195312, -0.15819549560546875, 0.015432357788085938, 0.25213623046875, -0.20703125, -0.1828765869140625, -0.1287384033203125, -0.090057373046875, 0.562103271484375, -0.09708786010742188, -0.5602798461914062, 0.3431396484375, 0.271392822265625, 0.231781005859375, -0.319091796875, -0.61199951171875, -0.699462890625, -0.25861549377441406, -0.19403839111328125, 0.239837646484375, 0.364898681640625, -0.0615081787109375, 0.36322021484375]]\n"
     ]
    }
   ],
   "source": [
    "str1 = \"A picture of Pixel, a male character with frumpy hair and small shades. He has a lanky and gangly build, with long limbs and a thin frame. He is wearing a hoodie and baggy pants, which add to his unkempt and disheveled appearance. His hair is a wild and unruly mess, with stray strands sticking up in all directions. He has a carefree and rebellious spirit, and seems to reject convention and authority. He is not particularly clean-cut or well-groomed, but has a raw and unpolished charm.\"\n",
    "str2 = \"Input A stunning figure adorned in a Pearl Kitsune mask, their hair a long and flowing Dreads of Obsidian. They stand tall in a Void head, looking out with a Half-open Jasper gaze, wearing a Track Jacket of Citrine, their body clad in an Azurite cloak, their Type is Y0K-A1, their Background is Azurite, their Body is Azurite, their Eyes are Half Open (Jasper), their Mouth is Neutral, their Wear is Track Jacket (Citrine), their Hair is Dreads (Obsidian), their Face is Kitsune Mask (Pearl), their Head is Void, their extra is Loop Earring, their Style is 5, their Strength is 3, their Spirit is 4.\"\n",
    "\n",
    "try:\n",
    "    tensor1 = tensorlize_texts(model, str1)\n",
    "except Exception as e:\n",
    "    tensor1 = tensorlize_texts_slideWindow(model, str1, 45, 20)\n",
    "\n",
    "print(tensor1)\n",
    "# print(len(tensor1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'picture', 'of', 'a', 'mysterious,', 'half-open', 'eye', 'stares', 'back', 'at', 'you', 'from', 'the', 'shadows.', 'A', 'man', 'in', 'a', 'Jasper', 'jasper', 'background,', 'with', 'a', 'frown', 'on', 'his', 'face,', 'wears', 'a', 'T-shirt', '(pearl)', 'and', 'has', 'long', '(turquoise)', 'hair,', 'a', 'pair', 'of', 'glasses', '(obsidian)', 'and', 'headphones', '(rose)', 'adorn', 'his', 'head.', 'The', '0N1', 'logo', 'pin', '(black)', 'is', 'pinned', 'to', 'his', 'chest.', 'He', 'exudes', 'a', 'strong,', 'confident', 'presence', 'with', 'a', 'spirit', '(5)', 'of', '10.']\n",
      "A picture of a mysterious, half-open eye stares back at you from the shadows. A man in a Jasper jasper background, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo\n",
      "50\n",
      "background, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10.\n",
      "49\n",
      "(obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10.\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "str1 = \"A picture of a mysterious, half-open eye stares back at you from the shadows. A man in a Jasper jasper background, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10.\"\n",
    "print(str1.split())\n",
    "splited_str1 = slide_window_tokenizer(str1, 50, 20)\n",
    "for i in splited_str1:\n",
    "    print(i)\n",
    "    print(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input A picture of a mysterious, half-open eye stares back at you from the shadows. A man in a Jasper jasper background, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10. is too long for context length 77",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m str2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbackground, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tensorlize_texts(model, str1)\n",
      "Cell \u001b[0;32mIn[15], line 97\u001b[0m, in \u001b[0;36mtensorlize_texts\u001b[0;34m(model, text_tokens_list)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtensorlize_texts\u001b[39m(model, text_tokens_list) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     88\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m    使用模型提取文本特征，返回文本特征向量列表\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m        torch.Tensor: 文本特征向量列表\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     text_tokens \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mtokenize(text_tokens_list)\u001b[39m.\u001b[39mcuda(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     99\u001b[0m         text_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_text(text_tokens)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/datassd2/sswang/NFT_Search/CLIP/utils/../clip/clip.py:234\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(texts, context_length, truncate)\u001b[0m\n\u001b[1;32m    232\u001b[0m             tokens[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m eot_token\n\u001b[1;32m    233\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtexts[i]\u001b[39m}\u001b[39;00m\u001b[39m is too long for context length \u001b[39m\u001b[39m{\u001b[39;00mcontext_length\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m     result[i, :\u001b[39mlen\u001b[39m(tokens)] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tokens)\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input A picture of a mysterious, half-open eye stares back at you from the shadows. A man in a Jasper jasper background, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10. is too long for context length 77"
     ]
    }
   ],
   "source": [
    "str2 = \"background, with a frown on his face, wears a T-shirt (pearl) and has long (turquoise) hair, a pair of glasses (obsidian) and headphones (rose) adorn his head. The 0N1 logo pin (black) is pinned to his chest. He exudes a strong, confident presence with a spirit (5) of 10.\"\n",
    "tensorlize_texts(model, str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFT_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
